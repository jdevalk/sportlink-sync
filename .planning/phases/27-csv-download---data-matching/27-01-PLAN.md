---
phase: 27-csv-download---data-matching
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - package-lock.json
  - .gitignore
  - download-nikki-contributions.js
  - lib/nikki-db.js
autonomous: true

must_haves:
  truths:
    - "CSV file downloads automatically after /leden table scrape completes"
    - "System extracts hoofdsom (total amount) from CSV for each member with valid nikki_id"
    - "Members without nikki_id in CSV are processed without errors (gracefully skipped)"
    - "CSV data correctly matches to /leden records by nikki_id"
  artifacts:
    - path: "download-nikki-contributions.js"
      provides: "CSV download and data merging"
      contains: "waitForEvent.*download"
    - path: "lib/nikki-db.js"
      provides: "hoofdsom column in schema and upsert"
      contains: "hoofdsom REAL"
  key_links:
    - from: "download-nikki-contributions.js"
      to: "csv-parse"
      via: "parse() streaming API"
      pattern: "require.*csv-parse"
    - from: "download-nikki-contributions.js"
      to: "lib/nikki-db.js"
      via: "upsertContributions with hoofdsom"
      pattern: "upsertContributions.*hoofdsom"
---

<objective>
Download CSV from Nikki Rapporten link after scraping /leden table and merge CSV data (hoofdsom) with HTML table data, matching by nikki_id.

Purpose: Enhance Nikki contribution data with hoofdsom (total amount) field from CSV that is not available in HTML table scrape.
Output: Updated download-nikki-contributions.js with CSV download capability, updated lib/nikki-db.js with hoofdsom column.
</objective>

<execution_context>
@/Users/joostdevalk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/joostdevalk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-csv-download---data-matching/27-RESEARCH.md
@download-nikki-contributions.js
@lib/nikki-db.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Setup csv-parse and gitignore</name>
  <files>package.json, .gitignore</files>
  <action>
    1. Install csv-parse library:
       ```bash
       npm install csv-parse
       ```

    2. Add downloads/ directory to .gitignore (add after "photos/" line):
       ```
       downloads/
       ```

    3. Add nikki-sync.sqlite to .gitignore (add in Data section):
       ```
       nikki-sync.sqlite
       ```
  </action>
  <verify>
    - `npm ls csv-parse` shows csv-parse installed
    - `grep downloads .gitignore` shows downloads/ entry
    - `grep nikki-sync .gitignore` shows nikki-sync.sqlite entry
  </verify>
  <done>csv-parse library is installed and temporary files are gitignored</done>
</task>

<task type="auto">
  <name>Task 2: Add CSV download capability</name>
  <files>download-nikki-contributions.js</files>
  <action>
    Modify download-nikki-contributions.js to download CSV after HTML table scrape.

    1. Add require at top:
       ```javascript
       const { parse } = require('csv-parse');
       ```

    2. Modify browser context creation (in runNikkiDownload) to enable downloads:
       ```javascript
       const context = await browser.newContext({
         acceptDownloads: true,  // REQUIRED for download operations
         userAgent: 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36'
       });
       ```

    3. Add new function to download and parse CSV (add after scrapeContributions function):
       ```javascript
       /**
        * Download CSV from Rapporten link and parse it.
        * Returns array of { nikki_id, lid_nr, hoofdsom, ... }
        */
       async function downloadAndParseCsv(page, logger) {
         logger.verbose('Starting CSV download from Rapporten link...');

         // Create downloads directory
         const downloadsDir = path.join(process.cwd(), 'downloads');
         await fs.mkdir(downloadsDir, { recursive: true });

         // CRITICAL: Set up download listener BEFORE clicking (race condition prevention)
         const downloadPromise = page.waitForEvent('download', { timeout: 30000 });

         // Click the Rapporten link - try multiple selectors for robustness
         const rapportenSelectors = [
           'a:has-text("Rapporten")',
           'button:has-text("Rapporten")',
           '[href*="rapport"]',
           'a[href*="export"]',
           '.export-btn'
         ];

         let clicked = false;
         for (const selector of rapportenSelectors) {
           try {
             const element = await page.$(selector);
             if (element) {
               await element.click();
               clicked = true;
               logger.verbose(`Clicked Rapporten using selector: ${selector}`);
               break;
             }
           } catch (e) {
             continue;
           }
         }

         if (!clicked) {
           logger.verbose('Could not find Rapporten link - CSV download skipped');
           return null;
         }

         // Wait for download to complete
         let download;
         try {
           download = await downloadPromise;
         } catch (e) {
           logger.verbose(`Download did not start within timeout: ${e.message}`);
           return null;
         }

         // Save file
         const suggestedFilename = download.suggestedFilename() || 'nikki-export.csv';
         const filePath = path.join(downloadsDir, suggestedFilename);
         await download.saveAs(filePath);
         logger.verbose(`CSV downloaded to: ${filePath}`);

         // Parse CSV
         const records = await new Promise((resolve, reject) => {
           const rows = [];
           require('fs').createReadStream(filePath)
             .pipe(parse({
               columns: true,           // Use first row as column names
               skip_empty_lines: true,
               trim: true,
               bom: true,               // Handle UTF-8 BOM
               relax_column_count: true // Handle inconsistent column counts
             }))
             .on('data', (row) => rows.push(row))
             .on('error', (err) => {
               logger.error(`CSV parse error: ${err.message}`);
               reject(err);
             })
             .on('end', () => resolve(rows));
         });

         logger.verbose(`Parsed ${records.length} rows from CSV`);

         // Log column names for debugging (first row)
         if (records.length > 0) {
           logger.verbose(`CSV columns: ${Object.keys(records[0]).join(', ')}`);
         }

         // Clean up file after parsing
         try {
           await fs.unlink(filePath);
           logger.verbose('Cleaned up CSV file');
         } catch (e) {
           logger.verbose(`Could not delete CSV file: ${e.message}`);
         }

         return records;
       }
       ```

    4. Add function to merge HTML and CSV data (add after downloadAndParseCsv):
       ```javascript
       /**
        * Merge HTML table data with CSV data by nikki_id.
        * CSV provides hoofdsom (total amount) not available in HTML.
        */
       function mergeHtmlAndCsvData(htmlRecords, csvRecords, logger) {
         if (!csvRecords || csvRecords.length === 0) {
           logger.verbose('No CSV data to merge - using HTML data only');
           return htmlRecords.map(r => ({ ...r, hoofdsom: null }));
         }

         // Build lookup map from CSV (nikki_id -> row)
         const csvMap = new Map();
         for (const csvRow of csvRecords) {
           // Try multiple possible column names for nikki_id
           const key = csvRow.nikki_id || csvRow.nikkiId || csvRow.nikki || csvRow.NikkiId;
           if (key) {
             csvMap.set(key, csvRow);
           }
         }

         logger.verbose(`Built CSV lookup map with ${csvMap.size} entries`);

         // Merge data
         let matchedCount = 0;
         let unmatchedCount = 0;

         const merged = htmlRecords.map(htmlRow => {
           const csvData = csvMap.get(htmlRow.nikki_id);

           if (csvData) {
             // Extract hoofdsom from CSV - try multiple column names
             const hoofdsomRaw = csvData.hoofdsom || csvData.Hoofdsom || csvData.total || csvData.Total || csvData.totaal || csvData.Totaal || '0';
             const hoofdsom = parseEuroAmount(hoofdsomRaw);
             matchedCount++;

             return {
               ...htmlRow,
               hoofdsom: hoofdsom
             };
           } else {
             // No match - gracefully set hoofdsom to null
             unmatchedCount++;
             return {
               ...htmlRow,
               hoofdsom: null
             };
           }
         });

         logger.verbose(`Merged: ${matchedCount} matched, ${unmatchedCount} unmatched (gracefully handled)`);
         return merged;
       }
       ```

    5. Update runNikkiDownload to call CSV download after scrape and merge data:
       In the try block after `const rawContributions = await scrapeContributions(page, logger);`, add:
       ```javascript
       // Download and parse CSV for additional data (hoofdsom)
       const csvRecords = await downloadAndParseCsv(page, logger);
       ```

       Then after the initial contributions mapping (where saldo is parsed), replace the mapping to include merge:
       ```javascript
       // Parse and validate contributions from HTML
       const htmlContributions = rawContributions.map((raw) => {
         const year = parseInt(raw.year, 10);
         const saldo = parseEuroAmount(raw.saldo_raw);

         return {
           knvb_id: raw.knvb_id,
           year: isNaN(year) ? 0 : year,
           nikki_id: raw.nikki_id,
           saldo: saldo,
           status: raw.status || null
         };
       }).filter((c) => c.knvb_id && c.year > 0 && c.nikki_id);

       // Merge with CSV data (adds hoofdsom field)
       const contributions = mergeHtmlAndCsvData(htmlContributions, csvRecords, logger);

       logger.verbose(`Parsed ${contributions.length} valid contributions`);
       ```
  </action>
  <verify>
    - File contains `require('csv-parse')`
    - File contains `acceptDownloads: true` in context options
    - File contains `downloadAndParseCsv` function
    - File contains `mergeHtmlAndCsvData` function
    - File contains `page.waitForEvent('download')` pattern
  </verify>
  <done>CSV download function added with proper Playwright download handling, CSV parsing, and data merge by nikki_id</done>
</task>

<task type="auto">
  <name>Task 3: Update database schema for hoofdsom</name>
  <files>lib/nikki-db.js</files>
  <action>
    Update lib/nikki-db.js to include hoofdsom column and update related functions.

    1. Update initDb() schema - add hoofdsom column after saldo:
       ```javascript
       function initDb(db) {
         db.exec(`
           CREATE TABLE IF NOT EXISTS nikki_contributions (
             id INTEGER PRIMARY KEY,
             knvb_id TEXT NOT NULL,
             year INTEGER NOT NULL,
             nikki_id TEXT NOT NULL,
             saldo REAL,
             hoofdsom REAL,
             status TEXT,
             source_hash TEXT NOT NULL,
             last_seen_at TEXT NOT NULL,
             created_at TEXT NOT NULL,
             UNIQUE(knvb_id, year)
           );

           CREATE INDEX IF NOT EXISTS idx_nikki_contributions_knvb_id
             ON nikki_contributions (knvb_id);

           CREATE INDEX IF NOT EXISTS idx_nikki_contributions_year
             ON nikki_contributions (year);

           CREATE INDEX IF NOT EXISTS idx_nikki_contributions_saldo
             ON nikki_contributions (saldo);
         `);

         // Migration: add hoofdsom column if it doesn't exist
         try {
           db.exec('ALTER TABLE nikki_contributions ADD COLUMN hoofdsom REAL');
         } catch (e) {
           // Column already exists, ignore
         }
       }
       ```

    2. Update computeContributionHash() to include hoofdsom:
       ```javascript
       function computeContributionHash(knvbId, year, nikkiId, saldo, hoofdsom, status) {
         const payload = stableStringify({
           knvb_id: knvbId,
           year: year,
           nikki_id: nikkiId,
           saldo: saldo,
           hoofdsom: hoofdsom,
           status: status
         });
         return crypto.createHash('sha256').update(payload).digest('hex');
       }
       ```

    3. Update upsertContributions() to handle hoofdsom:
       ```javascript
       function upsertContributions(db, contributions) {
         const now = new Date().toISOString();
         const stmt = db.prepare(`
           INSERT INTO nikki_contributions (
             knvb_id,
             year,
             nikki_id,
             saldo,
             hoofdsom,
             status,
             source_hash,
             last_seen_at,
             created_at
           )
           VALUES (
             @knvb_id,
             @year,
             @nikki_id,
             @saldo,
             @hoofdsom,
             @status,
             @source_hash,
             @last_seen_at,
             @created_at
           )
           ON CONFLICT(knvb_id, year) DO UPDATE SET
             nikki_id = excluded.nikki_id,
             saldo = excluded.saldo,
             hoofdsom = excluded.hoofdsom,
             status = excluded.status,
             source_hash = excluded.source_hash,
             last_seen_at = excluded.last_seen_at
         `);

         const insertMany = db.transaction((rows) => {
           rows.forEach((row) => stmt.run(row));
         });

         const rows = contributions.map((contrib) => ({
           knvb_id: contrib.knvb_id,
           year: contrib.year,
           nikki_id: contrib.nikki_id,
           saldo: contrib.saldo,
           hoofdsom: contrib.hoofdsom ?? null,
           status: contrib.status || null,
           source_hash: computeContributionHash(
             contrib.knvb_id,
             contrib.year,
             contrib.nikki_id,
             contrib.saldo,
             contrib.hoofdsom,
             contrib.status
           ),
           last_seen_at: now,
           created_at: now
         }));

         insertMany(rows);
       }
       ```

    4. Update all SELECT queries to include hoofdsom:

       In getContributionsByKnvbId():
       ```javascript
       const stmt = db.prepare(`
         SELECT knvb_id, year, nikki_id, saldo, hoofdsom, status
         FROM nikki_contributions
         WHERE knvb_id = ?
         ORDER BY year DESC
       `);
       ```

       In getContributionsByYear():
       ```javascript
       const stmt = db.prepare(`
         SELECT knvb_id, year, nikki_id, saldo, hoofdsom, status
         FROM nikki_contributions
         WHERE year = ?
         ORDER BY knvb_id ASC
       `);
       ```

       In getAllContributions():
       ```javascript
       const stmt = db.prepare(`
         SELECT knvb_id, year, nikki_id, saldo, hoofdsom, status
         FROM nikki_contributions
         ORDER BY year DESC, knvb_id ASC
       `);
       ```

       In getMembersWithOutstandingBalance():
       ```javascript
       const stmt = db.prepare(`
         SELECT knvb_id, year, nikki_id, saldo, hoofdsom, status
         FROM nikki_contributions
         WHERE saldo > 0
         ORDER BY saldo DESC, year DESC
       `);
       ```

       In getContributionsGroupedByMember():
       ```javascript
       const stmt = db.prepare(`
         SELECT knvb_id, year, nikki_id, saldo, hoofdsom, status
         FROM nikki_contributions
         ORDER BY knvb_id ASC, year DESC
       `);
       // ... and update the grouping to include hoofdsom:
       grouped.get(row.knvb_id).push({
         year: row.year,
         nikki_id: row.nikki_id,
         saldo: row.saldo,
         hoofdsom: row.hoofdsom,
         status: row.status
       });
       ```
  </action>
  <verify>
    - Schema contains `hoofdsom REAL` column
    - Migration block attempts ALTER TABLE for existing databases
    - computeContributionHash includes hoofdsom parameter
    - upsertContributions handles hoofdsom in INSERT and UPDATE
    - All SELECT queries include hoofdsom column
  </verify>
  <done>Database schema updated with hoofdsom column, all queries include the new field, migration handles existing databases</done>
</task>

</tasks>

<verification>
After completing all tasks:

1. **Installation check:**
   ```bash
   npm ls csv-parse
   ```
   Should show csv-parse in dependencies.

2. **Gitignore check:**
   ```bash
   grep -E "downloads|nikki-sync" .gitignore
   ```
   Should show both entries.

3. **Code structure check:**
   ```bash
   grep -l "acceptDownloads\|csv-parse\|hoofdsom" download-nikki-contributions.js lib/nikki-db.js
   ```
   Should return both files.

4. **Syntax check:**
   ```bash
   node -c download-nikki-contributions.js
   node -c lib/nikki-db.js
   ```
   Both should complete without syntax errors.
</verification>

<success_criteria>
1. csv-parse is installed as a dependency
2. downloads/ and nikki-sync.sqlite are in .gitignore
3. download-nikki-contributions.js:
   - Has acceptDownloads: true in browser context
   - Has downloadAndParseCsv function with proper event handling
   - Has mergeHtmlAndCsvData function matching by nikki_id
   - Gracefully handles missing Rapporten link or nikki_id
4. lib/nikki-db.js:
   - Schema includes hoofdsom REAL column
   - Migration adds column to existing databases
   - All CRUD operations include hoofdsom field
5. Both files pass syntax check (node -c)
</success_criteria>

<output>
After completion, create `.planning/phases/27-csv-download---data-matching/27-01-SUMMARY.md`
</output>
